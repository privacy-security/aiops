{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "# sns.set(context='paper', style='whitegrid', color_codes=True)   \n",
    "sns.set_palette(sns.color_palette([\"#017b92\", \"#f97306\", \"#0485d1\"]))  # [\"jade green\", \"orange\", \"blue\"] \n",
    "\n",
    "# @stevo trik, ked mas viac notebookov, aby ti jeden nb nezozral celu GPU pamat (per_process_gpu_memory_fraction):\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.compat.v1.keras.backend as K\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-offense",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_teacher_forcing = False  # incremental learning\n",
    "cfg_remove_peak = False\n",
    "\n",
    "cfg_sequence_len = 24        # default=24\n",
    "cfg_sequence_len_y = 10\n",
    "cfg_steps_ahead = 1\n",
    "\n",
    "cfg_batch_size = 1\n",
    "cfg_units = 48              # default=48 GRU units\n",
    "cfg_dropout_rate = 0.6      # default=0.5\n",
    "\n",
    "cfg_num_epochs = 100\n",
    "cfg_num_epochs_update = 2    # incremental learning\n",
    "cfg_epochs_patience = 10\n",
    "\n",
    "cfg_fig_size_x = 20\n",
    "cfg_fig_size_y = 5\n",
    " \n",
    "data_train_filename = 'data/data_train.tsv'\n",
    "data_test_filename = 'data/data_test.tsv'\n",
    "\n",
    "model_name = 'models/mods2_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-developer",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe to numpy array\n",
    "def read_data(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t', skiprows=0, skipfooter=0, engine='python')\n",
    "    data = df.values\n",
    "    print('read_data: ', filename, '\\t', data.shape[1], data.dtype, '\\n', list(df))\n",
    "    return data\n",
    "\n",
    "\n",
    "# data is numpy array\n",
    "def transform(data, epsilon=1, remove_peak=cfg_remove_peak):\n",
    "    if remove_peak:\n",
    "        # InterQuartile Range (IQR)\n",
    "        q_min, q_max = np.percentile(data, [25, 75], axis=0)\n",
    "        iqr = q_max - q_min\n",
    "        iqr_min = q_min - 1.5*iqr\n",
    "        iqr_max = q_max + 1.5*iqr\n",
    "        data = np.clip(data, a_min=iqr_min, a_max=iqr_max)\n",
    "    data = np.where(data < 0, epsilon, data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Scale all metrics but each separately: normalization or standardization\n",
    "def normalize(data, scaler=None):\n",
    "    if not scaler:\n",
    "        # scaler = Pipeline([\n",
    "        #    ('PowerTransformer', PowerTransformer()),\n",
    "        #    ('MinMaxScaler', MinMaxScaler(feature_range=(0,1))),\n",
    "        #    ('QuantileTransformer', QuantileTransformer(output_distribution='normal', n_quantiles=100)),\n",
    "        #])\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        norm_data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        norm_data = scaler.transform(data)\n",
    "    # print('\\nnormalize:', norm_data.shape)\n",
    "    return norm_data, scaler\n",
    "\n",
    "\n",
    "def make_timeseries(data,\n",
    "                    sequence_len=cfg_sequence_len,\n",
    "                    sequence_len_y=cfg_sequence_len_y,\n",
    "                    steps_ahead=cfg_steps_ahead\n",
    "                    ):\n",
    "    data_x = data_y = data\n",
    "\n",
    "    if sequence_len_y > 1:\n",
    "        for i in range(1, sequence_len_y):\n",
    "            data_y = np.column_stack((data_y[:-1], data[i:]))\n",
    "        data_x = data_x[:-(sequence_len_y-1)]\n",
    "\n",
    "    if steps_ahead > 1:\n",
    "        data_x = data_x[:-(steps_ahead-1)]\n",
    "        data_y = data_y[steps_ahead-1:]\n",
    "\n",
    "    tsg_data = TimeseriesGenerator(data_x, data_y, length=sequence_len,\n",
    "                                   sampling_rate=1, stride=1, batch_size=cfg_batch_size)\n",
    "    # x, y = tsg_data[0]\n",
    "    # print('\\ttsg x.shape=', x.shape, '\\n\\tx=', x, '\\n\\ttsg y.shape=', y.shape, '\\n\\ty=', y)\n",
    "    return tsg_data\n",
    "\n",
    "\n",
    "def transform_invert(data, denorm, sequence_len=cfg_sequence_len, steps_ahead=cfg_steps_ahead):\n",
    "    begin = sequence_len + steps_ahead -1           # indexing is from 0\n",
    "    end = begin + len(denorm)\n",
    "    Y = data[begin:end]                             # excludes the end index\n",
    "    return denorm, Y\n",
    "\n",
    "\n",
    "def fit_model(data_train, data_test, model, epochs, scaler, callbacks_list, teacher_forcing=cfg_teacher_forcing):\n",
    "    trans_train = transform(data_train)\n",
    "    norm_train, _ = normalize(trans_train, scaler)\n",
    "    tsg_train = make_timeseries(norm_train)\n",
    "    \n",
    "    if teacher_forcing:\n",
    "        for i in range(epochs):\n",
    "            history = model.fit(tsg_train, epochs=1, batch_size=cfg_batch_size, shuffle=False, callbacks=callbacks_list)\n",
    "            model.reset_states()\n",
    "    else:\n",
    "        trans_test = transform(data_test)\n",
    "        norm_test, _ = normalize(trans_test, scaler)\n",
    "        tsg_test = make_timeseries(norm_test)\n",
    "        history = model.fit(tsg_train, epochs=epochs, callbacks=callbacks_list, validation_data=tsg_test)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def predict(data_test, model, scaler):\n",
    "    trans_test = transform(data_test)\n",
    "    norm_test, _ = normalize(trans_test, scaler)\n",
    "    tsg_test = make_timeseries(norm_test)\n",
    "    return model.predict(tsg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-maintenance",
   "metadata": {},
   "source": [
    "## Functions: evaluation + plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mods_utils\n",
    "\n",
    "def eval_predictions(pred_test, Y_test, model_type):\n",
    "    print('\\nEvaluation with real values - One step')\n",
    "    results = [model_type]\n",
    "\n",
    "    err_train = err_test = 0\n",
    "    for m in ['SMAPE', 'MAPE', 'RMSE', 'R2', 'COSINE']:\n",
    "        if m == 'SMAPE':\n",
    "            err_test  = mods_utils.smape(Y_test, pred_test)\n",
    "        elif m == 'RMSE':\n",
    "            err_test  = mods_utils.rmse(Y_test, pred_test)\n",
    "        elif m == 'R2':\n",
    "            err_test  = mods_utils.r2(Y_test, pred_test)\n",
    "        elif m == 'COSINE':\n",
    "            err_test  = mods_utils.cosine(Y_test, pred_test)\n",
    "        results.append([m, err_train, err_test])\n",
    "\n",
    "    line = results[0]                   # model_type\n",
    "    for r in results[1:]:\n",
    "        line += '\\t' + r[0] + '\\t'      # SMAPE, MAPE, R2, COSINE\n",
    "        line += '\\t'.join(x if isinstance(x, str) else str(\"{0:0.4f}\".format(x)) for x in r[2])  # test\n",
    "    print(line)\n",
    "    return line\n",
    "\n",
    "\n",
    "def plot_predictions(pred_test, Y_test, multivariate,\n",
    "                     fig_x=cfg_fig_size_x,\n",
    "                     fig_y=cfg_fig_size_y\n",
    "                     ):\n",
    "    plt.rcParams[\"figure.figsize\"] = (fig_x, fig_y)\n",
    "    if multivariate > 1:\n",
    "        fig, ax = plt.subplots(multivariate, sharex=False, figsize=(fig_x, multivariate*fig_y))\n",
    "        for i in range(multivariate):\n",
    "            ax[i].plot(Y_test[:, i])\n",
    "            ax[i].plot(pred_test[:, i])\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(fig_x, multivariate*fig_y))\n",
    "        ax.plot(Y_test[:, 0])\n",
    "        ax.plot(pred_test[:, 0])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('models/plot_image', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # np.savetxt(cfg.app_data_plot + \"pred_test.tsv\", pred_test, delimiter='\\t')\n",
    "    # np.savetxt(cfg.app_data_plot + \"Y_test.tsv\", Y_test, delimiter='\\t')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-throw",
   "metadata": {},
   "source": [
    "## Train data + scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = read_data(data_train_filename)\n",
    "trans_train = transform(data_train)\n",
    "norm_train, scaler = normalize(trans_train)\n",
    "\n",
    "# save scaler\n",
    "scaler_filename = model_name + '.scaler'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print('Scaler saved to: ', scaler_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-lunch",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = read_data(data_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-spectacular",
   "metadata": {},
   "source": [
    "## Create + compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model typ: GRU')\n",
    "multivariate = data_train.shape[1]\n",
    "\n",
    "if cfg_teacher_forcing:\n",
    "    x = Input(batch_shape=(cfg_batch_size, cfg_sequence_len, multivariate))\n",
    "    h = GRU(units=cfg.units, stateful=True, return_sequences=True)(x)         # activation='tanh'\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = GRU(units=cfg.units, stateful=True, return_sequences=False)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    y = Dense(units=multivariate*cfg_sequence_len_y, activation='sigmoid')(h)    \n",
    "else:\n",
    "    x = Input(shape=(cfg_sequence_len, multivariate)) \n",
    "# without dropout it is overfitting \n",
    "\n",
    "# GRU\n",
    "    h = GRU(units=cfg_units, return_sequences=True)(x)     \n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = GRU(units=cfg_units, return_sequences=False)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    '''\n",
    "# seq2seqGRU \n",
    "    h = GRU(units=cfg_units)(x)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = RepeatVector(cfg_sequence_len)(h)\n",
    "    h = GRU(units=cfg_units)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "# CNN-GRU\n",
    "    h = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "    h = MaxPooling1D(pool_size=2)(h)\n",
    "    h = GRU(units=cfg_units, return_sequences=False)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "# LSTM - similar to LSTM just longer training\n",
    "    h = LSTM(units=cfg_units, return_sequences=True)(x)     \n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = LSTM(units=cfg_units, return_sequences=False)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "# CNN           \n",
    "    h = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "    h = MaxPooling1D(pool_size=2)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = Conv1D(filters=128, kernel_size=3, activation='relu')(h)\n",
    "    h = MaxPooling1D(pool_size=2)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = Conv1D(filters=128, kernel_size=3, activation='relu')(h)\n",
    "    h = MaxPooling1D(pool_size=2)(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = Flatten()(h)\n",
    "# autoencoder\n",
    "    nn = [256, 128, 64, 32, 16, 32, 64, 256]\n",
    "    h = Dense(units=nn[0], activation='relu')(x)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    for n in nn[1:]:\n",
    "        h = Dense(units=n, activation='relu')(h)\n",
    "        h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = Flatten()(h)\n",
    "# MLP\n",
    "    h = Dense(units=cfg_units, \n",
    "              activation='elu', \n",
    "              kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "              bias_regularizer=regularizers.l2(1e-4),\n",
    "              activity_regularizer=regularizers.l2(1e-5)\n",
    "             )(x)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = Dense(units=cfg_units, \n",
    "              activation='elu', \n",
    "              kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "              bias_regularizer=regularizers.l2(1e-4),\n",
    "              activity_regularizer=regularizers.l2(1e-5)\n",
    "             )(h)\n",
    "    h = Dropout(cfg_dropout_rate)(h)\n",
    "    h = Dense(units=cfg_units, \n",
    "              activation='elu', \n",
    "              kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "              bias_regularizer=regularizers.l2(1e-4),\n",
    "              activity_regularizer=regularizers.l2(1e-5)\n",
    "             )(h)\n",
    "    h = Flatten()(h)\n",
    "    '''      \n",
    "    # Adding the output layer:\n",
    "    y = Dense(units=multivariate*cfg_sequence_len_y, activation='sigmoid')(h)\n",
    "    \n",
    "model = Model(inputs=x, outputs=y)\n",
    "print(model.summary())\n",
    "\n",
    "# compile model\n",
    "opt = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mse', 'mae'])     # 'cosine', 'mape'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-likelihood",
   "metadata": {},
   "source": [
    "## Fit + save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "earlystops = EarlyStopping(monitor='loss', patience=cfg_epochs_patience, verbose=1, restore_best_weights=True)\n",
    "callbacks_list = [earlystops]\n",
    "\n",
    "# fit model\n",
    "model, history = fit_model(data_train, data_test, model, cfg_num_epochs, scaler, callbacks_list)\n",
    "\n",
    "# save model\n",
    "model.save(model_name)\n",
    "print('\\nSave trained model: ', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# print(history.history.keys())\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-slovak",
   "metadata": {},
   "source": [
    "## Predict and Update: Incremental or Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg_teacher_forcing:\n",
    "    data_train_copy = data_train\n",
    "    data_test_copy = data_test\n",
    "    # data_test_copy = data_test[0:50,:]\n",
    "\n",
    "    padding = cfg_sequence_len + cfg_steps_ahead + cfg_sequence_len_y -1\n",
    "    pred_model = None\n",
    "\n",
    "    for i in range(data_test_copy.shape[0] - padding):\n",
    "        if i > 0:\n",
    "            fit_model(data_train_copy, model, cfg_num_epochs_update, scaler, callbacks_list)\n",
    "        data = data_test[i:i+padding,:]\n",
    "        pred = predict(data, model, scaler)\n",
    "        pred_model = np.vstack((pred_model, pred)) if pred_model is not None else pred\n",
    "        \n",
    "        # TODO: peak detection\n",
    "        data = transform(data)\n",
    "        data_train_copy = np.vstack((data_train_copy, data))    \n",
    "        print(i)\n",
    "        \n",
    "    # Save incremental model\n",
    "    model_name = model_name + '_incremental'\n",
    "    model.save(model_name)\n",
    "    print('\\nSave trained model: ', model_name)\n",
    "    \n",
    "    # Evaluation + plot (incremental)\n",
    "    eval_line = ''\n",
    "    for i in range(cfg_sequence_len_y):\n",
    "        one_y_test = pred_model[:, i * multivariate:(i+1) * multivariate]\n",
    "        denorm_test = scaler.inverse_transform(one_y_test)\n",
    "        pred_test, Y_test = transform_invert(data_test, denorm_test, cfg_sequence_len, cfg_steps_ahead)\n",
    "\n",
    "        # Evaluate with real values\n",
    "        eval_line += str(i+1) + '\\t' + eval_predictions(pred_test, Y_test, 'LSTM') + '\\n'\n",
    "\n",
    "        # Plot\n",
    "        plot_predictions(pred_test, Y_test, multivariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-weather",
   "metadata": {},
   "source": [
    "## Predict + Evaluation + Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-trail",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_model = predict(data_test, model, scaler)\n",
    "\n",
    "eval_line = ''\n",
    "for i in range(cfg_sequence_len_y):\n",
    "    one_y_test = pred_model[:, i * multivariate:(i+1) * multivariate]\n",
    "    denorm_test = scaler.inverse_transform(one_y_test)\n",
    "    pred_test, Y_test = transform_invert(data_test, denorm_test, cfg_sequence_len, cfg_steps_ahead)\n",
    "\n",
    "    # Evaluate with real values\n",
    "    eval_line += str(i+1) + '\\t' + eval_predictions(pred_test, Y_test, 'NN') + '\\n'\n",
    "    \n",
    "# Plot\n",
    "if cfg_sequence_len_y == 1:\n",
    "    plot_predictions(pred_test, Y_test, multivariate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
