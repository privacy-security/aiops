{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twelve-cleaners",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 11:22:35.980933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 11:22:36.429698: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2023-03-24 11:22:36.429741: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2023-03-24 11:22:36.429746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "# sns.set(context='paper', style='whitegrid', color_codes=True)   \n",
    "sns.set_palette(sns.color_palette([\"#017b92\", \"#f97306\", \"#0485d1\"]))  # [\"jade green\", \"orange\", \"blue\"] \n",
    "\n",
    "import mods_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-offense",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "presidential-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_teacher_forcing = False  # incremental learning\n",
    "cfg_transformer = True\n",
    "cfg_remove_peak = False\n",
    "\n",
    "cfg_sequence_len = 24        # default=24\n",
    "cfg_sequence_len_y = 1\n",
    "cfg_steps_ahead = 1\n",
    "\n",
    "cfg_stacks = 2\n",
    "cfg_rnn_units = 48          # default=48 GRU units\n",
    "cfg_mlp_units = 128\n",
    "\n",
    "cfg_dropout_rate = 0.3      # default=0.5\n",
    "cfg_batch_size = 1\n",
    "cfg_num_epochs = 100\n",
    "cfg_num_epochs_update = 2    # incremental learning\n",
    "cfg_epochs_patience = 10\n",
    "\n",
    "cfg_fig_size_x = 20\n",
    "cfg_fig_size_y = 5\n",
    " \n",
    "data_train_filename = 'data/data_train.tsv'\n",
    "data_test_filename = 'data/data_test.tsv'\n",
    "\n",
    "model_name = 'models/mods2_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-developer",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "careful-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe to numpy array\n",
    "def read_data(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t', skiprows=0, skipfooter=0, engine='python')\n",
    "    data = df.values\n",
    "    print('read_data: ', filename, '\\t', data.shape[1], data.dtype, '\\n', list(df))\n",
    "    return data\n",
    "\n",
    "\n",
    "# data is numpy array\n",
    "def transform(data, epsilon=1, remove_peak=cfg_remove_peak):\n",
    "    if remove_peak:\n",
    "        # InterQuartile Range (IQR)\n",
    "        q_min, q_max = np.percentile(data, [25, 75], axis=0)\n",
    "        iqr = q_max - q_min\n",
    "        iqr_min = q_min - 1.5*iqr\n",
    "        iqr_max = q_max + 1.5*iqr\n",
    "        data = np.clip(data, a_min=iqr_min, a_max=iqr_max)\n",
    "    data = np.where(data < 0, epsilon, data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Scale all metrics but each separately: normalization or standardization\n",
    "def normalize(data, scaler=None):\n",
    "    if not scaler:\n",
    "        # scaler = Pipeline([\n",
    "        #    ('PowerTransformer', PowerTransformer()),\n",
    "        #    ('MinMaxScaler', MinMaxScaler(feature_range=(0,1))),\n",
    "        #    ('QuantileTransformer', QuantileTransformer(output_distribution='normal', n_quantiles=100)),\n",
    "        #])\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        norm_data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        norm_data = scaler.transform(data)\n",
    "    # print('\\nnormalize:', norm_data.shape)\n",
    "    return norm_data, scaler\n",
    "\n",
    "\n",
    "def make_timeseries(data,\n",
    "                    sequence_len=cfg_sequence_len,\n",
    "                    sequence_len_y=cfg_sequence_len_y,\n",
    "                    steps_ahead=cfg_steps_ahead\n",
    "                    ):\n",
    "    data_x = data_y = data\n",
    "\n",
    "    if sequence_len_y > 1:\n",
    "        for i in range(1, sequence_len_y):\n",
    "            data_y = np.column_stack((data_y[:-1], data[i:]))\n",
    "        data_x = data_x[:-(sequence_len_y-1)]\n",
    "\n",
    "    if steps_ahead > 1:\n",
    "        data_x = data_x[:-(steps_ahead-1)]\n",
    "        data_y = data_y[steps_ahead-1:]\n",
    "\n",
    "    tsg_data = TimeseriesGenerator(data_x, data_y, length=sequence_len,\n",
    "                                   sampling_rate=1, stride=1, batch_size=cfg_batch_size)\n",
    "    # x, y = tsg_data[0]\n",
    "    # print('\\ttsg x.shape=', x.shape, '\\n\\tx=', x, '\\n\\ttsg y.shape=', y.shape, '\\n\\ty=', y)\n",
    "    return tsg_data\n",
    "\n",
    "\n",
    "def transform_invert(data, denorm, sequence_len=cfg_sequence_len, steps_ahead=cfg_steps_ahead):\n",
    "    begin = sequence_len + steps_ahead -1           # indexing is from 0\n",
    "    end = begin + len(denorm)\n",
    "    Y = data[begin:end]                             # excludes the end index\n",
    "    return denorm, Y\n",
    "\n",
    "\n",
    "def fit_model(data_train, data_test, model, epochs, scaler, callbacks_list, teacher_forcing=cfg_teacher_forcing):\n",
    "    trans_train = transform(data_train)\n",
    "    norm_train, _ = normalize(trans_train, scaler)\n",
    "    tsg_train = make_timeseries(norm_train)\n",
    "    \n",
    "    if teacher_forcing:\n",
    "        for i in range(epochs):\n",
    "            history = model.fit(tsg_train, epochs=1, batch_size=cfg_batch_size, shuffle=False, callbacks=callbacks_list)\n",
    "            model.reset_states()\n",
    "    else:\n",
    "        trans_test = transform(data_test)\n",
    "        norm_test, _ = normalize(trans_test, scaler)\n",
    "        tsg_test = make_timeseries(norm_test)\n",
    "        history = model.fit(tsg_train, epochs=epochs, callbacks=callbacks_list, validation_data=tsg_test)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def predict(data_test, model, scaler):\n",
    "    trans_test = transform(data_test)\n",
    "    norm_test, _ = normalize(trans_test, scaler)\n",
    "    tsg_test = make_timeseries(norm_test)\n",
    "    return model.predict(tsg_test)\n",
    "\n",
    "def eval_predictions(pred_test, Y_test, model_type):\n",
    "    print('\\nEvaluation with real values - One step')\n",
    "    results = [model_type]\n",
    "\n",
    "    err_train = err_test = 0\n",
    "    for m in ['SMAPE', 'MAPE', 'RMSE', 'R2', 'COSINE']:\n",
    "        if m == 'SMAPE':\n",
    "            err_test  = mods_utils.smape(Y_test, pred_test)\n",
    "        elif m == 'RMSE':\n",
    "            err_test  = mods_utils.rmse(Y_test, pred_test)\n",
    "        elif m == 'R2':\n",
    "            err_test  = mods_utils.r2(Y_test, pred_test)\n",
    "        elif m == 'COSINE':\n",
    "            err_test  = mods_utils.cosine(Y_test, pred_test)\n",
    "        results.append([m, err_train, err_test])\n",
    "\n",
    "    line = results[0]                   # model_type\n",
    "    for r in results[1:]:\n",
    "        line += '\\t' + r[0] + '\\t'      # SMAPE, MAPE, R2, COSINE\n",
    "        line += '\\t'.join(x if isinstance(x, str) else str(\"{0:0.4f}\".format(x)) for x in r[2])  # test\n",
    "    print(line)\n",
    "    return line\n",
    "\n",
    "def plot_predictions(pred_test, Y_test, multivariate,\n",
    "                     fig_x=cfg_fig_size_x,\n",
    "                     fig_y=cfg_fig_size_y\n",
    "                     ):\n",
    "    plt.rcParams[\"figure.figsize\"] = (fig_x, fig_y)\n",
    "    if multivariate > 1:\n",
    "        fig, ax = plt.subplots(multivariate, sharex=False, figsize=(fig_x, multivariate*fig_y))\n",
    "        for i in range(multivariate):\n",
    "            ax[i].plot(Y_test[:, i])\n",
    "            ax[i].plot(pred_test[:, i])\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(fig_x, multivariate*fig_y))\n",
    "        ax.plot(Y_test[:, 0])\n",
    "        ax.plot(pred_test[:, 0])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('models/plot_image', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # np.savetxt(cfg.app_data_plot + \"pred_test.tsv\", pred_test, delimiter='\\t')\n",
    "    # np.savetxt(cfg.app_data_plot + \"Y_test.tsv\", Y_test, delimiter='\\t')\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-throw",
   "metadata": {},
   "source": [
    "## Train data + scaler, Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "diagnostic-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_data:  data/data_train.tsv \t 5 int64 \n",
      " ['conn_count_uid_in', 'conn_count_uid_out', 'dns_count_uid_out', 'http_count_uid_in', 'ssl_count_uid_in']\n",
      "Scaler saved to:  models/mods2_model.scaler\n",
      "read_data:  data/data_test.tsv \t 5 int64 \n",
      " ['conn_count_uid_in', 'conn_count_uid_out', 'dns_count_uid_out', 'http_count_uid_in', 'ssl_count_uid_in']\n"
     ]
    }
   ],
   "source": [
    "data_train = read_data(data_train_filename)\n",
    "trans_train = transform(data_train)\n",
    "norm_train, scaler = normalize(trans_train)\n",
    "\n",
    "# save scaler\n",
    "scaler_filename = model_name + '.scaler'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print('Scaler saved to: ', scaler_filename)\n",
    "\n",
    "data_test = read_data(data_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5349393",
   "metadata": {},
   "source": [
    "## Transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea5a78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, \n",
    "        num_heads=num_heads, \n",
    "        dropout=dropout,\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    # outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    outputs = layers.Dense(units=multivariate*cfg_sequence_len_y, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-spectacular",
   "metadata": {},
   "source": [
    "## Create + compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "little-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model typ: Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 11:22:37.210411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.210916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.215619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.216069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.216506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.216938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.217766: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 11:22:37.380237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.380671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.381059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.381438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.381817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.382202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.992251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.992690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.993073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.993443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.993811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.994212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9631 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2023-03-24 11:22:37.994420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-24 11:22:37.994763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9621 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 24, 5)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 24, 5)       10          ['input_2[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 24, 5)       11781       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 24, 5)        0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 24, 5)       0           ['dropout[0][0]',                \n",
      " da)                                                              'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 24, 5)       10          ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 24, 4)        24          ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 24, 4)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 24, 5)        25          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 24, 5)       0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 24, 5)       10          ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 24, 5)       11781       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 24, 5)        0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 24, 5)       0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 24, 5)       10          ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 24, 4)        24          ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 24, 4)        0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 24, 5)        25          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 24, 5)       0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 24, 5)       10          ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 24, 5)       11781       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 24, 5)        0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 24, 5)       0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 24, 5)       10          ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 24, 4)        24          ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 24, 4)        0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 24, 5)        25          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 24, 5)       0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 24, 5)       10          ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 24, 5)       11781       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 24, 5)        0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 24, 5)       0           ['dropout_6[0][0]',              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 24, 5)       10          ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 24, 4)        24          ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 24, 4)        0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 24, 5)        25          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 24, 5)       0           ['conv1d_7[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 24)          0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          6400        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 5)            1285        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,085\n",
      "Trainable params: 55,085\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Model typ: Transformer')\n",
    "multivariate = data_train.shape[1]\n",
    "\n",
    "if cfg_teacher_forcing:\n",
    "    x = layers.Input(batch_shape=(cfg_batch_size, cfg_sequence_len, multivariate))\n",
    "    h = layers.GRU(units=cfg.units, stateful=True, return_sequences=True)(x)         # activation='tanh'\n",
    "    h = layers.Dropout(cfg_dropout_rate)(h)\n",
    "    h = layers.GRU(units=cfg.units, stateful=True, return_sequences=False)(h)\n",
    "    h = layers.Dropout(cfg_dropout_rate)(h)\n",
    "    y = layers.Dense(units=multivariate*cfg_sequence_len_y, activation='sigmoid')(h)    \n",
    "elif cfg_transformer:\n",
    "    x = layers.Input(shape=(cfg_sequence_len, multivariate))\n",
    "    input_shape = x.shape[1:]\n",
    "    \n",
    "    model = build_model(\n",
    "        input_shape,\n",
    "        head_size=cfg_mlp_units,\n",
    "        num_heads=4,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[256],\n",
    "        mlp_dropout=cfg_dropout_rate,\n",
    "        dropout=cfg_dropout_rate,\n",
    "    )\n",
    "else:\n",
    "    x = layers.Input(shape=(cfg_sequence_len, multivariate)) \n",
    "    # GRU\n",
    "    h = layers.GRU(units=cfg_rnn_units, return_sequences=True)(x)     \n",
    "    h = layers.Dropout(cfg_dropout_rate)(h)\n",
    "    h = layers.GRU(units=cfg_rnn_units, return_sequences=False)(h)\n",
    "    h = layers.Dropout(cfg_dropout_rate)(h)\n",
    "    # Adding the output layer:\n",
    "    y = layers.Dense(units=multivariate*cfg_sequence_len_y, activation='sigmoid')(h)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "# compile model\n",
    "loss ='mean_squared_error'\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "metrics=['mse', 'mae'],     # 'cosine', 'mape'\n",
    "# loss = \"sparse_categorical_crossentropy\"\n",
    "# opt = keras.optimizers.Adam(learning_rate=1e-4),\n",
    "# metrics = [\"sparse_categorical_accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-likelihood",
   "metadata": {},
   "source": [
    "## Fit + save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-niagara",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 11:22:42.295001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-24 11:22:42.678763: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f5928866bf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-24 11:22:42.678776: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-03-24 11:22:42.678780: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-03-24 11:22:42.681599: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-24 11:22:42.764121: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402/1402 [==============================] - 27s 13ms/step - loss: 0.0165 - mse: 0.0165 - mae: 0.0870 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0594\n",
      "Epoch 2/100\n",
      "1402/1402 [==============================] - 17s 12ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0767 - val_loss: 0.0084 - val_mse: 0.0084 - val_mae: 0.0644\n",
      "Epoch 3/100\n",
      "1402/1402 [==============================] - 17s 12ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0750 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0877\n",
      "Epoch 4/100\n",
      "1402/1402 [==============================] - 18s 13ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0733 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0638\n",
      "Epoch 5/100\n",
      "1402/1402 [==============================] - 17s 12ms/step - loss: 0.0112 - mse: 0.0112 - mae: 0.0720 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0645\n",
      "Epoch 6/100\n",
      "1402/1402 [==============================] - 19s 14ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0700 - val_loss: 0.0080 - val_mse: 0.0080 - val_mae: 0.0653\n",
      "Epoch 7/100\n",
      "1402/1402 [==============================] - 16s 11ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0686 - val_loss: 0.0075 - val_mse: 0.0075 - val_mae: 0.0538\n",
      "Epoch 8/100\n",
      "1402/1402 [==============================] - 17s 12ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0670 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0513\n",
      "Epoch 9/100\n",
      "1402/1402 [==============================] - 19s 13ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0671 - val_loss: 0.0067 - val_mse: 0.0067 - val_mae: 0.0541\n",
      "Epoch 10/100\n",
      "1402/1402 [==============================] - 15s 11ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0670 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0523\n",
      "Epoch 11/100\n",
      "1402/1402 [==============================] - 16s 11ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0655 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0548\n",
      "Epoch 12/100\n",
      "1402/1402 [==============================] - 17s 12ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0631 - val_loss: 0.0068 - val_mse: 0.0068 - val_mae: 0.0577\n",
      "Epoch 13/100\n",
      "1402/1402 [==============================] - 16s 12ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0640 - val_loss: 0.0065 - val_mse: 0.0065 - val_mae: 0.0509\n",
      "Epoch 14/100\n",
      "1402/1402 [==============================] - 15s 11ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0637 - val_loss: 0.0066 - val_mse: 0.0066 - val_mae: 0.0560\n",
      "Epoch 15/100\n",
      " 241/1402 [====>.........................] - ETA: 10s - loss: 0.0102 - mse: 0.0102 - mae: 0.0665"
     ]
    }
   ],
   "source": [
    "# tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "earlystops = EarlyStopping(\n",
    "    monitor='loss', \n",
    "    patience=cfg_epochs_patience, \n",
    "    verbose=1, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "callbacks_list = [earlystops]\n",
    "\n",
    "# fit model\n",
    "model, history = fit_model(\n",
    "    data_train, \n",
    "    data_test, \n",
    "    model, \n",
    "    cfg_num_epochs, scaler, callbacks_list\n",
    ")\n",
    "\n",
    "# model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# save model\n",
    "model.save(model_name)\n",
    "print('\\nSave trained model: ', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# print(history.history.keys())\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-slovak",
   "metadata": {},
   "source": [
    "## Predict and Update: Incremental or Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg_teacher_forcing:\n",
    "    data_train_copy = data_train\n",
    "    data_test_copy = data_test\n",
    "    # data_test_copy = data_test[0:50,:]\n",
    "\n",
    "    padding = cfg_sequence_len + cfg_steps_ahead + cfg_sequence_len_y -1\n",
    "    pred_model = None\n",
    "\n",
    "    for i in range(data_test_copy.shape[0] - padding):\n",
    "        if i > 0:\n",
    "            fit_model(data_train_copy, model, cfg_num_epochs_update, scaler, callbacks_list)\n",
    "        data = data_test[i:i+padding,:]\n",
    "        pred = predict(data, model, scaler)\n",
    "        pred_model = np.vstack((pred_model, pred)) if pred_model is not None else pred\n",
    "        \n",
    "        # TODO: peak detection\n",
    "        data = transform(data)\n",
    "        data_train_copy = np.vstack((data_train_copy, data))    \n",
    "        print(i)\n",
    "        \n",
    "    # Save incremental model\n",
    "    model_name = model_name + '_incremental'\n",
    "    model.save(model_name)\n",
    "    print('\\nSave trained model: ', model_name)\n",
    "    \n",
    "    # Evaluation + plot (incremental)\n",
    "    eval_line = ''\n",
    "    for i in range(cfg_sequence_len_y):\n",
    "        one_y_test = pred_model[:, i * multivariate:(i+1) * multivariate]\n",
    "        denorm_test = scaler.inverse_transform(one_y_test)\n",
    "        pred_test, Y_test = transform_invert(data_test, denorm_test, cfg_sequence_len, cfg_steps_ahead)\n",
    "\n",
    "        # Evaluate with real values\n",
    "        eval_line += str(i+1) + '\\t' + eval_predictions(pred_test, Y_test, 'LSTM') + '\\n'\n",
    "\n",
    "        # Plot\n",
    "        plot_predictions(pred_test, Y_test, multivariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-weather",
   "metadata": {},
   "source": [
    "## Predict + Evaluation + Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-trail",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_model = predict(data_test, model, scaler)\n",
    "\n",
    "eval_line = ''\n",
    "for i in range(cfg_sequence_len_y):\n",
    "    one_y_test = pred_model[:, i * multivariate:(i+1) * multivariate]\n",
    "    denorm_test = scaler.inverse_transform(one_y_test)\n",
    "    pred_test, Y_test = transform_invert(data_test, denorm_test, cfg_sequence_len, cfg_steps_ahead)\n",
    "\n",
    "    # Evaluate with real values\n",
    "    eval_line += str(i+1) + '\\t' + eval_predictions(pred_test, Y_test, 'NN') + '\\n'\n",
    "    \n",
    "# Plot\n",
    "if cfg_sequence_len_y == 1:\n",
    "    plot_predictions(pred_test, Y_test, multivariate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
